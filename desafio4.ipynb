{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "579b027a",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/hernancontigiani/ceia_memorias_especializacion/raw/master/Figures/logoFIUBA.jpg\" width=\"500\" align=\"center\">\n",
    "\n",
    "\n",
    "# Procesamiento de lenguaje natural I\n",
    "\n",
    "# LSTM Bot QA\n",
    "\n",
    "### Datos\n",
    "\n",
    "### El objecto es utilizar datos disponibles del challenge ConvAI2 (Conversational Intelligence Challenge 2) de conversaciones en inglÃ©s. Se construirÃ¡ un BOT para responder a preguntas del usuario (QA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ea09fd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --no-cache-dir gdown --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464baaeb",
   "metadata": {},
   "source": [
    "#### 2 - Preprocesamiento\n",
    "Realizar el preprocesamiento necesario para obtener:\n",
    "\n",
    "word2idx_inputs, max_input_len\n",
    "word2idx_outputs, max_out_len, num_words_output\n",
    "encoder_input_sequences, decoder_output_sequences, decoder_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5765375a",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "446f6a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, io, json, gzip, zipfile, gdown, heapq, re, unicodedata\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.utils import pad_sequences, register_keras_serializable\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.layers import Activation, Dropout, Dense, LSTM, Input, Concatenate, Embedding, Attention, Bidirectional, LayerNormalization\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b6b86a",
   "metadata": {},
   "source": [
    "### DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ea7a4970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El dataset ya se encuentra descargado\n"
     ]
    }
   ],
   "source": [
    "# Descargar la carpeta de dataset\n",
    "if os.access('data_volunteers.json', os.F_OK) is False:\n",
    "    url = 'https://drive.google.com/uc?id=1awUxYwImF84MIT5-jCaYAPe2QwSgS1hN&export=download'\n",
    "    output = 'data_volunteers.json'\n",
    "    gdown.download(url, output, quiet=False)\n",
    "else:\n",
    "    print(\"El dataset ya se encuentra descargado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d214b2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_file = \"data_volunteers.json\"\n",
    "with open(text_file) as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5c5c509a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['dialog', 'start_time', 'end_time', 'bot_profile', 'user_profile', 'eval_score', 'profile_match', 'participant1_id', 'participant2_id'])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observar los campos disponibles en cada linea del dataset\n",
    "data[0].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175a883b",
   "metadata": {},
   "source": [
    "### 2 - Preprocesamiento\n",
    "#### Realizar el preprocesamiento necesario para obtener:\n",
    "\n",
    "#### word2idx_inputs, max_input_len\n",
    "#### word2idx_outputs, max_out_len, num_words_output\n",
    "#### encoder_input_sequences, decoder_output_sequences, decoder_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ba63757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de rows utilizadas: 6033\n"
     ]
    }
   ],
   "source": [
    "chat_in = []\n",
    "chat_out = []\n",
    "\n",
    "input_sentences = []\n",
    "output_sentences = []\n",
    "output_sentences_inputs = []\n",
    "max_len = 30\n",
    "\n",
    "def clean_text(txt):\n",
    "    txt = txt.lower()    \n",
    "    txt.replace(\"\\'d\", \" had\")\n",
    "    txt.replace(\"\\'s\", \" is\")\n",
    "    txt.replace(\"\\'m\", \" am\")\n",
    "    txt.replace(\"don't\", \"do not\")\n",
    "    txt = re.sub(r'\\W+', ' ', txt)\n",
    "    \n",
    "    return txt\n",
    "\n",
    "for line in data:\n",
    "    for i in range(len(line['dialog'])-1):\n",
    "        # vamos separando el texto en \"preguntas\" (chat_in)\n",
    "        # y \"respuestas\" (chat_out)\n",
    "        chat_in = clean_text(line['dialog'][i]['text'])\n",
    "        chat_out = clean_text(line['dialog'][i+1]['text'])\n",
    "\n",
    "        if len(chat_in) >= max_len or len(chat_out) >= max_len:\n",
    "            continue\n",
    "\n",
    "        input_sentence, output = chat_in, chat_out\n",
    "        \n",
    "        # output sentence (decoder_output) tiene \n",
    "        output_sentence = output + ' '\n",
    "        # output sentence input (decoder_input) tiene \n",
    "        output_sentence_input = ' ' + output\n",
    "\n",
    "        input_sentences.append(input_sentence)\n",
    "        output_sentences.append(output_sentence)\n",
    "        output_sentences_inputs.append(output_sentence_input)\n",
    "\n",
    "print(\"Cantidad de rows utilizadas:\", len(input_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "187c0c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('hi how are you ', 'not bad and you  ', ' not bad and you ')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_sentences[1], output_sentences[1], output_sentences_inputs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87c98e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6398"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalizaciÃ³n bÃ¡sica\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = unicodedata.normalize(\"NFKC\", s).lower()\n",
    "    # espacios alrededor de puntuaciÃ³n comÃºn\n",
    "    s = re.sub(r\"([?.!,;:()\\\"'])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"\\s{2,}\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# tokenizaciÃ³n simple por espacios (tras normalizar)\n",
    "def tokenize(s: str):\n",
    "    return normalize_text(s).split()\n",
    "\n",
    "# Extraer pares Human -> Bot (pregunta/respuesta)\n",
    "pairs = []\n",
    "for conv in data:\n",
    "    dialog = conv.get(\"dialog\", [])\n",
    "    # Recorremos turnos consecutivos Human -> Bot\n",
    "    for i in range(len(dialog)-1):\n",
    "        a, b = dialog[i], dialog[i+1]\n",
    "        if a.get(\"sender_class\",\"\").lower() == \"human\" and b.get(\"sender_class\",\"\").lower() == \"bot\":\n",
    "            inp = a.get(\"text\",\"\").strip()\n",
    "            out = b.get(\"text\",\"\").strip()\n",
    "            if inp and out:\n",
    "                pairs.append((inp, out))\n",
    "\n",
    "len(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fce83375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Hello!', 'Hi! How are you?'),\n",
       " ('Not bad! And You?',\n",
       "  \"I'm doing well. Just got engaged to my high school sweetheart.\"),\n",
       " ('Wowowowow! Congratulations! Is she pretty?',\n",
       "  \"She 's pretty cute. She invited me to dinner tonight. ðŸ™‚\")]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vista rÃ¡pida de algunos pares\n",
    "pairs[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ab3e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3295, 1825, 298, 102, 1825)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construir vocabularios independientes (inputs/outputs)\n",
    "SPECIAL_TOKENS = [\"<pad>\", \"<unk>\", \"<sos>\", \"<eos>\"]\n",
    "PAD, UNK, SOS, EOS = SPECIAL_TOKENS\n",
    "\n",
    "tok_inputs = [tokenize(x) for x,_ in pairs]\n",
    "tok_outputs = [tokenize(y) for _,y in pairs]\n",
    "\n",
    "# Vocabularios por frecuencia\n",
    "cnt_in = Counter(itertools.chain.from_iterable(tok_inputs))\n",
    "cnt_out = Counter(itertools.chain.from_iterable(tok_outputs))\n",
    "\n",
    "# Orden por frecuencia y luego alfabÃ©tico para estabilidad\n",
    "def build_word2idx(counter):\n",
    "    words = [w for w,_ in sorted(counter.items(), key=lambda kv: (-kv[1], kv[0]))]\n",
    "    word2idx = {PAD:0, UNK:1, SOS:2, EOS:3}\n",
    "    for i,w in enumerate(words, start=len(SPECIAL_TOKENS)):\n",
    "        word2idx[w] = i\n",
    "    return word2idx\n",
    "\n",
    "word2idx_inputs = build_word2idx(cnt_in)\n",
    "word2idx_outputs = build_word2idx(cnt_out)\n",
    "\n",
    "max_input_len = max(len(t) for t in tok_inputs) if tok_inputs else 0\n",
    "max_out_len_base = max(len(t) for t in tok_outputs) if tok_outputs else 0\n",
    "# Para el decoder: aÃ±adiremos <sos> y <eos>, por lo que el largo mÃ¡ximo cambia:\n",
    "max_out_len = max_out_len_base + 2  # incluye <sos> y <eos>\n",
    "\n",
    "num_words_output = len(word2idx_outputs)\n",
    "\n",
    "(len(word2idx_inputs), len(word2idx_outputs), max_input_len, max_out_len, num_words_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b66f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          input_text  \\\n",
      "0                                            hello !   \n",
      "1                                not bad ! and you ?   \n",
      "2      wowowowow ! congratulations ! is she pretty ?   \n",
      "3  cool ! have a good time you both ! and what is...   \n",
      "4                 me too . and what about iggy pop ?   \n",
      "5                              hey ? where are you ?   \n",
      "6                         i ' m playing pipe organ .   \n",
      "7                                               hi !   \n",
      "8  cool ! i ' m going to finish with my homework ...   \n",
      "9                                              bro ?   \n",
      "\n",
      "                                         output_text  \\\n",
      "0                                 hi ! how are you ?   \n",
      "1  i ' m doing well . just got engaged to my high...   \n",
      "2  she ' s pretty cute . she invited me to dinner...   \n",
      "3             i love music ! i love taylor swift . ðŸ˜‰   \n",
      "4  i love ziggy ! he is my favorite . are you and...   \n",
      "5  i am sorry to hear that . what do you do for f...   \n",
      "6  that sounds impressive . i like to go out to e...   \n",
      "7                        hello , how are you today ?   \n",
      "8     i am doing great . i just got home from work .   \n",
      "9                               where are you from ?   \n",
      "\n",
      "                                             enc_seq  \\\n",
      "0                                           [60, 22]   \n",
      "1                            [43, 226, 22, 18, 6, 5]   \n",
      "2               [3222, 22, 671, 22, 17, 249, 297, 5]   \n",
      "3  [71, 22, 24, 8, 33, 64, 6, 791, 22, 18, 11, 17...   \n",
      "4              [25, 55, 7, 18, 11, 42, 1211, 733, 5]   \n",
      "5                              [77, 5, 54, 15, 6, 5]   \n",
      "6                      [4, 9, 20, 140, 2630, 612, 7]   \n",
      "7                                           [35, 22]   \n",
      "8  [71, 22, 4, 9, 20, 143, 13, 503, 52, 19, 2149,...   \n",
      "9                                           [241, 5]   \n",
      "\n",
      "                                             dec_out  \\\n",
      "0                          [2, 55, 31, 20, 16, 7, 6]   \n",
      "1  [2, 5, 23, 33, 48, 53, 4, 56, 78, 946, 12, 35,...   \n",
      "2  [2, 248, 23, 64, 281, 467, 4, 248, 1446, 146, ...   \n",
      "3        [2, 5, 21, 68, 31, 5, 21, 384, 458, 4, 162]   \n",
      "4  [2, 5, 21, 1153, 31, 152, 17, 35, 47, 4, 16, 7...   \n",
      "5  [2, 5, 15, 90, 12, 128, 22, 4, 10, 8, 7, 8, 18...   \n",
      "6  [2, 22, 74, 1442, 4, 5, 14, 12, 61, 205, 12, 1...   \n",
      "7                      [2, 45, 13, 20, 16, 7, 57, 6]   \n",
      "8    [2, 5, 15, 48, 66, 4, 5, 56, 78, 83, 42, 39, 4]   \n",
      "9                             [2, 115, 16, 7, 42, 6]   \n",
      "\n",
      "                                             dec_tgt  \n",
      "0                          [55, 31, 20, 16, 7, 6, 3]  \n",
      "1  [5, 23, 33, 48, 53, 4, 56, 78, 946, 12, 35, 33...  \n",
      "2  [248, 23, 64, 281, 467, 4, 248, 1446, 146, 12,...  \n",
      "3        [5, 21, 68, 31, 5, 21, 384, 458, 4, 162, 3]  \n",
      "4  [5, 21, 1153, 31, 152, 17, 35, 47, 4, 16, 7, 6...  \n",
      "5  [5, 15, 90, 12, 128, 22, 4, 10, 8, 7, 8, 18, 5...  \n",
      "6  [22, 74, 1442, 4, 5, 14, 12, 61, 205, 12, 148,...  \n",
      "7                      [45, 13, 20, 16, 7, 57, 6, 3]  \n",
      "8    [5, 15, 48, 66, 4, 5, 56, 78, 83, 42, 39, 4, 3]  \n",
      "9                             [115, 16, 7, 42, 6, 3]  \n",
      "Guardado en: ./desafio4/preproc_convai2\n",
      "encoder_input_sequences: 6398\n",
      "decoder_output_sequences: 6398\n",
      "decoder_targets: 6398\n"
     ]
    }
   ],
   "source": [
    "def encode(tokens, w2i):\n",
    "    return [w2i.get(tok, w2i[\"<unk>\"]) for tok in tokens]\n",
    "\n",
    "# --- Secuencias del encoder\n",
    "encoder_input_sequences = [encode(toks, word2idx_inputs) for toks in tok_inputs]\n",
    "\n",
    "decoder_output_sequences = [encode([\"<sos>\"] + toks, word2idx_outputs) for toks in tok_outputs]\n",
    "decoder_targets = [encode(toks + [\"<eos>\"], word2idx_outputs) for toks in tok_outputs]\n",
    "\n",
    "# --- Guardado en disco ---\n",
    "out_dir = \"./desafio4/preproc_convai2\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "# Diccionarios\n",
    "with open(os.path.join(out_dir, \"word2idx_inputs.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word2idx_inputs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(os.path.join(out_dir, \"word2idx_outputs.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(word2idx_outputs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Secuencias\n",
    "np.savez_compressed(\n",
    "    os.path.join(out_dir, \"sequences.npz\"),\n",
    "    encoder_input_sequences=np.array(encoder_input_sequences, dtype=object),\n",
    "    decoder_output_sequences=np.array(decoder_output_sequences, dtype=object),\n",
    "    decoder_targets=np.array(decoder_targets, dtype=object),\n",
    "    allow_pickle=True\n",
    ")\n",
    "\n",
    "# --- Vista previa\n",
    "preview_rows = 10\n",
    "df_preview = pd.DataFrame({\n",
    "    \"input_text\": [\" \".join(t) for t in tok_inputs[:preview_rows]],\n",
    "    \"output_text\": [\" \".join(t) for t in tok_outputs[:preview_rows]],\n",
    "    \"enc_seq\": encoder_input_sequences[:preview_rows],\n",
    "    \"dec_out\": decoder_output_sequences[:preview_rows],\n",
    "    \"dec_tgt\": decoder_targets[:preview_rows],\n",
    "})\n",
    "\n",
    "# Mostrar por pantalla\n",
    "print(df_preview.head(10))\n",
    "\n",
    "# Guardar a CSV\n",
    "df_preview.to_csv(os.path.join(out_dir, \"preview_first10.csv\"), index=False, encoding=\"utf-8\")\n",
    "\n",
    "# PequeÃ±o resumen\n",
    "print(\"Guardado en:\", out_dir)\n",
    "print(\"encoder_input_sequences:\", len(encoder_input_sequences))\n",
    "print(\"decoder_output_sequences:\", len(decoder_output_sequences))\n",
    "print(\"decoder_targets:\", len(decoder_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5aa9df6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'num_pairs': 6398,\n",
       " 'vocab_inputs': 3295,\n",
       " 'vocab_outputs': 1825,\n",
       " 'max_input_len_tokens': 298,\n",
       " 'max_output_len_tokens_including_sos_eos': 102,\n",
       " 'avg_input_len': 5.225382932166302,\n",
       " 'avg_output_len': 10.611284776492655}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Resumen estadÃ­stico\n",
    "stats = {\n",
    "    \"num_pairs\": len(pairs),\n",
    "    \"vocab_inputs\": len(word2idx_inputs),\n",
    "    \"vocab_outputs\": len(word2idx_outputs),\n",
    "    \"max_input_len_tokens\": max_input_len,\n",
    "    \"max_output_len_tokens_including_sos_eos\": max_out_len,\n",
    "    \"avg_input_len\": float(np.mean([len(t) for t in tok_inputs])),\n",
    "    \"avg_output_len\": float(np.mean([len(t) for t in tok_outputs])),\n",
    "}\n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e365ba9c",
   "metadata": {},
   "source": [
    "#### 3 - Preparar los embeddings\n",
    "Utilizar los embeddings de Glove o FastText para transformar los tokens de entrada en vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ab8294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando embeddings en: ./embeddings\\glove.6B.100d.txt\n",
      "Vocab encoder: 3295 palabras\n",
      "Embeddings encontrados para 2884/3295 palabras (87.53% cobertura)\n",
      "Filas con vector preentrenado: 2884/3295 (87.53%)\n",
      "Guardado: ./desafio4/embeddings\\encoder_embedding_matrix_glove100.npy\n",
      "Metadatos: encoder_embedding_matrix_glove100_meta.json\n"
     ]
    }
   ],
   "source": [
    "# ========= Config =========\n",
    "EMB_TYPE = \"glove\"\n",
    "EMB_DIM  = 100\n",
    "EMB_DIR  = \"./embeddings\"\n",
    "VOCAB_PATH = \"./desafio4/preproc_convai2/word2idx_inputs.json\"\n",
    "SAVE_DIR  = \"./desafio4/embeddings\"\n",
    "\n",
    "os.makedirs(EMB_DIR, exist_ok=True)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "GLOVE_URL   = \"https://nlp.stanford.edu/data/glove.6B.zip\"\n",
    "FASTTEXT_URL = \"https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\"\n",
    "\n",
    "# ========= Utilidades =========\n",
    "def download(url, dest):\n",
    "    print(f\"Descargando: {url}\")\n",
    "    urllib.request.urlretrieve(url, dest)\n",
    "    print(f\"Guardado en: {dest}\")\n",
    "\n",
    "def ensure_glove(dim=100):\n",
    "    zip_path = os.path.join(EMB_DIR, \"glove.6B.zip\")\n",
    "    target_txt = os.path.join(EMB_DIR, f\"glove.6B.{dim}d.txt\")\n",
    "    if not os.path.exists(target_txt):\n",
    "        if not os.path.exists(zip_path):\n",
    "            download(GLOVE_URL, zip_path)\n",
    "        print(\"Descomprimiendo GloVe...\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            zf.extract(f\"glove.6B.{dim}d.txt\", EMB_DIR)\n",
    "    return target_txt\n",
    "\n",
    "def ensure_fasttext_vec():\n",
    "    zip_path = os.path.join(EMB_DIR, \"wiki-news-300d-1M.vec.zip\")\n",
    "    target_vec = os.path.join(EMB_DIR, \"wiki-news-300d-1M.vec\")\n",
    "    if not os.path.exists(target_vec):\n",
    "        if not os.path.exists(zip_path):\n",
    "            download(FASTTEXT_URL, zip_path)\n",
    "        print(\"Descomprimiendo FastText...\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            zf.extract(\"wiki-news-300d-1M.vec\", EMB_DIR)\n",
    "    return target_vec\n",
    "\n",
    "def open_text_maybe_gzip(path):\n",
    "    if path.endswith(\".gz\"):\n",
    "        return io.TextIOWrapper(gzip.open(path, \"rb\"), encoding=\"utf-8\", errors=\"ignore\")\n",
    "    return open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\")\n",
    "\n",
    "def maybe_skip_header(line):\n",
    "    parts = line.rstrip().split()\n",
    "    if len(parts) == 2:\n",
    "        try:\n",
    "            int(parts[0]); int(parts[1])\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "# ========= Asegurar archivo de embeddings =========\n",
    "if EMB_TYPE == \"glove\":\n",
    "    emb_path = ensure_glove(EMB_DIM)\n",
    "elif EMB_TYPE == \"fasttext\":\n",
    "    EMB_DIM = 300\n",
    "    emb_path = ensure_fasttext_vec()\n",
    "else:\n",
    "    raise ValueError(\"EMB_TYPE debe ser 'glove' o 'fasttext'.\")\n",
    "\n",
    "print(\"Usando embeddings en:\", emb_path)\n",
    "\n",
    "# ========= Cargar vocabulario del encoder =========\n",
    "with open(VOCAB_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx_inputs = json.load(f)\n",
    "\n",
    "PAD = \"<pad>\"; UNK = \"<unk>\"\n",
    "vocab_size = len(word2idx_inputs)\n",
    "print(f\"Vocab encoder: {vocab_size} palabras\")\n",
    "\n",
    "# ========= Leer embeddings sÃ³lo para palabras del vocab =========\n",
    "embeddings = {}\n",
    "loaded = 0\n",
    "with open_text_maybe_gzip(emb_path) as f:\n",
    "    first = True\n",
    "    for line in f:\n",
    "        if first and EMB_TYPE == \"fasttext\" and maybe_skip_header(line):\n",
    "            first = False\n",
    "            continue\n",
    "        first = False\n",
    "\n",
    "        parts = line.rstrip().split(\" \")\n",
    "        if len(parts) < EMB_DIM + 1:\n",
    "            continue\n",
    "        word = parts[0]\n",
    "        if word in word2idx_inputs:\n",
    "            try:\n",
    "                vec = np.asarray(parts[1:1+EMB_DIM], dtype=np.float32)\n",
    "                if vec.size == EMB_DIM:\n",
    "                    embeddings[word] = vec\n",
    "                    loaded += 1\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "print(f\"Embeddings encontrados para {loaded}/{vocab_size} palabras ({loaded/vocab_size:.2%} cobertura)\")\n",
    "\n",
    "# ========= Construir matriz =========\n",
    "rng = np.random.default_rng(123)\n",
    "embedding_matrix = rng.normal(0.0, 0.01, size=(vocab_size, EMB_DIM)).astype(np.float32)\n",
    "\n",
    "# pad -> ceros\n",
    "if PAD in word2idx_inputs:\n",
    "    embedding_matrix[word2idx_inputs[PAD]] = np.zeros((EMB_DIM,), dtype=np.float32)\n",
    "\n",
    "# unk -> media\n",
    "mean_vec = np.mean(np.stack(list(embeddings.values())), axis=0) if embeddings else np.zeros((EMB_DIM,), dtype=np.float32)\n",
    "if UNK in word2idx_inputs:\n",
    "    embedding_matrix[word2idx_inputs[UNK]] = mean_vec\n",
    "\n",
    "# asignar conocidas\n",
    "hit = 0\n",
    "for w, idx in word2idx_inputs.items():\n",
    "    if w in (PAD, UNK): \n",
    "        continue\n",
    "    vec = embeddings.get(w)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[idx] = vec\n",
    "        hit += 1\n",
    "\n",
    "print(f\"Filas con vector preentrenado: {hit}/{vocab_size} ({hit/vocab_size:.2%})\")\n",
    "\n",
    "# ========= Guardar =========\n",
    "fname = f\"encoder_embedding_matrix_{EMB_TYPE}{EMB_DIM}.npy\"\n",
    "save_path = os.path.join(SAVE_DIR, fname)\n",
    "np.save(save_path, embedding_matrix)\n",
    "\n",
    "meta = {\n",
    "    \"type\": EMB_TYPE,\n",
    "    \"dim\": EMB_DIM,\n",
    "    \"vocab_size\": vocab_size,\n",
    "    \"pad_index\": word2idx_inputs.get(PAD, None),\n",
    "    \"unk_index\": word2idx_inputs.get(UNK, None),\n",
    "    \"coverage_known_tokens\": int(hit),\n",
    "    \"coverage_ratio\": float(hit / vocab_size)\n",
    "}\n",
    "with open(os.path.join(SAVE_DIR, fname.replace(\".npy\", \"_meta.json\")), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(\"Guardado:\", save_path)\n",
    "print(\"Metadatos:\", fname.replace(\".npy\", \"_meta.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fe2faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.load(\"./desafio4/embeddings/encoder_embedding_matrix_glove100.npy\")\n",
    "emb_layer = Embedding(\n",
    "    input_dim=W.shape[0],\n",
    "    output_dim=W.shape[1],\n",
    "    weights=[W],\n",
    "    trainable=False,\n",
    "    mask_zero=True  # asume <pad>=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d627b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.tensor(np.load(\"./desafio4/embeddings/encoder_embedding_matrix_glove100.npy\"), dtype=torch.float32)\n",
    "emb_layer = nn.Embedding.from_pretrained(W, freeze=True, padding_idx=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19a76b",
   "metadata": {},
   "source": [
    "#### 4 - Entrenar el modelo\n",
    "Entrenar un modelo basado en el esquema encoder-decoder utilizando los datos generados en los puntos anteriores. Utilce como referencias los ejemplos vistos en clase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ddec54a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p99 longitudes -> input: 24 | output: 60\n",
      "Usando embeddings preentrenados del encoder: ./desafio4/embeddings\\encoder_embedding_matrix_glove100.npy\n",
      "Epoch 1/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.2408 - accuracy: 0.1296\n",
      "Epoch 1: val_loss improved from inf to 0.18105, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 27s 252ms/step - loss: 0.2408 - accuracy: 0.1296 - val_loss: 0.1811 - val_accuracy: 0.1581 - lr: 0.0020\n",
      "Epoch 2/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1897 - accuracy: 0.2047\n",
      "Epoch 2: val_loss improved from 0.18105 to 0.16669, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 23s 256ms/step - loss: 0.1897 - accuracy: 0.2047 - val_loss: 0.1667 - val_accuracy: 0.2343 - lr: 0.0020\n",
      "Epoch 3/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.3090\n",
      "Epoch 3: val_loss improved from 0.16669 to 0.14428, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 23s 253ms/step - loss: 0.1674 - accuracy: 0.3090 - val_loss: 0.1443 - val_accuracy: 0.3904 - lr: 0.0020\n",
      "Epoch 4/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1442 - accuracy: 0.4368\n",
      "Epoch 4: val_loss improved from 0.14428 to 0.13145, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 23s 252ms/step - loss: 0.1442 - accuracy: 0.4368 - val_loss: 0.1315 - val_accuracy: 0.4571 - lr: 0.0020\n",
      "Epoch 5/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1305 - accuracy: 0.5026\n",
      "Epoch 5: val_loss improved from 0.13145 to 0.12501, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 23s 251ms/step - loss: 0.1305 - accuracy: 0.5026 - val_loss: 0.1250 - val_accuracy: 0.4926 - lr: 0.0020\n",
      "Epoch 6/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1208 - accuracy: 0.5429\n",
      "Epoch 6: val_loss improved from 0.12501 to 0.11999, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 23s 251ms/step - loss: 0.1208 - accuracy: 0.5429 - val_loss: 0.1200 - val_accuracy: 0.5119 - lr: 0.0020\n",
      "Epoch 7/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1141 - accuracy: 0.5691\n",
      "Epoch 7: val_loss improved from 0.11999 to 0.11710, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 23s 250ms/step - loss: 0.1141 - accuracy: 0.5691 - val_loss: 0.1171 - val_accuracy: 0.5280 - lr: 0.0020\n",
      "Epoch 8/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1097 - accuracy: 0.5841\n",
      "Epoch 8: val_loss improved from 0.11710 to 0.11575, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 23s 252ms/step - loss: 0.1097 - accuracy: 0.5841 - val_loss: 0.1157 - val_accuracy: 0.5323 - lr: 0.0020\n",
      "Epoch 9/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.5999\n",
      "Epoch 9: val_loss improved from 0.11575 to 0.11503, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 23s 253ms/step - loss: 0.1055 - accuracy: 0.5999 - val_loss: 0.1150 - val_accuracy: 0.5327 - lr: 0.0020\n",
      "Epoch 10/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.1023 - accuracy: 0.6145\n",
      "Epoch 10: val_loss did not improve from 0.11503\n",
      "90/90 [==============================] - 23s 252ms/step - loss: 0.1023 - accuracy: 0.6145 - val_loss: 0.1156 - val_accuracy: 0.5349 - lr: 0.0020\n",
      "Epoch 11/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0991 - accuracy: 0.6307\n",
      "Epoch 11: val_loss improved from 0.11503 to 0.11412, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 23s 252ms/step - loss: 0.0991 - accuracy: 0.6307 - val_loss: 0.1141 - val_accuracy: 0.5461 - lr: 0.0020\n",
      "Epoch 12/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0961 - accuracy: 0.6452\n",
      "Epoch 12: val_loss did not improve from 0.11412\n",
      "90/90 [==============================] - 22s 249ms/step - loss: 0.0961 - accuracy: 0.6452 - val_loss: 0.1149 - val_accuracy: 0.5432 - lr: 0.0020\n",
      "Epoch 13/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.6591\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0010000000474974513.\n",
      "\n",
      "Epoch 13: val_loss did not improve from 0.11412\n",
      "90/90 [==============================] - 23s 250ms/step - loss: 0.0936 - accuracy: 0.6591 - val_loss: 0.1145 - val_accuracy: 0.5459 - lr: 0.0020\n",
      "Epoch 14/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0886 - accuracy: 0.6900\n",
      "Epoch 14: val_loss improved from 0.11412 to 0.11391, saving model to ./desafio4/models_tied_p99_clean\\best_phase1.keras\n",
      "90/90 [==============================] - 22s 249ms/step - loss: 0.0886 - accuracy: 0.6900 - val_loss: 0.1139 - val_accuracy: 0.5515 - lr: 0.0010\n",
      "Epoch 15/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0865 - accuracy: 0.7044\n",
      "Epoch 15: val_loss did not improve from 0.11391\n",
      "90/90 [==============================] - 23s 253ms/step - loss: 0.0865 - accuracy: 0.7044 - val_loss: 0.1146 - val_accuracy: 0.5510 - lr: 0.0010\n",
      "Epoch 16/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0848 - accuracy: 0.7153\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "\n",
      "Epoch 16: val_loss did not improve from 0.11391\n",
      "90/90 [==============================] - 22s 250ms/step - loss: 0.0848 - accuracy: 0.7153 - val_loss: 0.1157 - val_accuracy: 0.5458 - lr: 0.0010\n",
      "Epoch 17/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0817 - accuracy: 0.7384\n",
      "Epoch 17: val_loss did not improve from 0.11391\n",
      "90/90 [==============================] - 23s 251ms/step - loss: 0.0817 - accuracy: 0.7384 - val_loss: 0.1153 - val_accuracy: 0.5471 - lr: 5.0000e-04\n",
      "Epoch 18/30\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0805 - accuracy: 0.7462\n",
      "Epoch 18: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "Restoring model weights from the end of the best epoch: 14.\n",
      "\n",
      "Epoch 18: val_loss did not improve from 0.11391\n",
      "90/90 [==============================] - 23s 252ms/step - loss: 0.0805 - accuracy: 0.7462 - val_loss: 0.1159 - val_accuracy: 0.5438 - lr: 5.0000e-04\n",
      "Epoch 18: early stopping\n",
      "Epoch 1/8\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0860 - accuracy: 0.6922\n",
      "Epoch 1: val_loss improved from inf to 0.11385, saving model to ./desafio4/models_tied_p99_clean\\best_phase2.keras\n",
      "90/90 [==============================] - 28s 256ms/step - loss: 0.0860 - accuracy: 0.6922 - val_loss: 0.1138 - val_accuracy: 0.5582 - lr: 5.0000e-04\n",
      "Epoch 2/8\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0840 - accuracy: 0.7210\n",
      "Epoch 2: val_loss did not improve from 0.11385\n",
      "90/90 [==============================] - 22s 246ms/step - loss: 0.0840 - accuracy: 0.7210 - val_loss: 0.1144 - val_accuracy: 0.5529 - lr: 5.0000e-04\n",
      "Epoch 3/8\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0826 - accuracy: 0.7303\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.11385\n",
      "90/90 [==============================] - 22s 244ms/step - loss: 0.0826 - accuracy: 0.7303 - val_loss: 0.1151 - val_accuracy: 0.5499 - lr: 5.0000e-04\n",
      "Epoch 4/8\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.7482Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 4: val_loss did not improve from 0.11385\n",
      "90/90 [==============================] - 22s 245ms/step - loss: 0.0802 - accuracy: 0.7482 - val_loss: 0.1150 - val_accuracy: 0.5509 - lr: 2.5000e-04\n",
      "Epoch 4: early stopping\n",
      "Epoch 1/6\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0827 - accuracy: 0.7152\n",
      "Epoch 1: val_loss improved from inf to 0.11384, saving model to ./desafio4/models_tied_p99_clean\\best_phase3.keras\n",
      "90/90 [==============================] - 29s 275ms/step - loss: 0.0827 - accuracy: 0.7152 - val_loss: 0.1138 - val_accuracy: 0.5574 - lr: 2.0000e-04\n",
      "Epoch 2/6\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0823 - accuracy: 0.7336\n",
      "Epoch 2: ReduceLROnPlateau reducing learning rate to 9.999999747378752e-05.\n",
      "\n",
      "Epoch 2: val_loss did not improve from 0.11384\n",
      "90/90 [==============================] - 23s 260ms/step - loss: 0.0823 - accuracy: 0.7336 - val_loss: 0.1142 - val_accuracy: 0.5535 - lr: 2.0000e-04\n",
      "Epoch 3/6\n",
      "90/90 [==============================] - ETA: 0s - loss: 0.0813 - accuracy: 0.7415\n",
      "Epoch 3: ReduceLROnPlateau reducing learning rate to 4.999999873689376e-05.\n",
      "Restoring model weights from the end of the best epoch: 1.\n",
      "\n",
      "Epoch 3: val_loss did not improve from 0.11384\n",
      "90/90 [==============================] - 23s 259ms/step - loss: 0.0813 - accuracy: 0.7415 - val_loss: 0.1142 - val_accuracy: 0.5534 - lr: 1.0000e-04\n",
      "Epoch 3: early stopping\n",
      "Modelos guardados en: ./desafio4/models_tied_p99_clean\n"
     ]
    }
   ],
   "source": [
    "# ---------- Reproducibilidad ----------\n",
    "K.clear_session()\n",
    "np.random.seed(42); tf.random.set_seed(42)\n",
    "\n",
    "# ---------- Rutas ----------\n",
    "PREPROC_DIR = \"./desafio4/preproc_convai2\"\n",
    "EMB_DIR     = \"./desafio4/embeddings\"\n",
    "MODEL_DIR   = \"./desafio4/models_tied_p99_clean\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# ---------- Cargar vocab y secuencias ----------\n",
    "with open(os.path.join(PREPROC_DIR, \"word2idx_inputs.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx_inputs = json.load(f)\n",
    "with open(os.path.join(PREPROC_DIR, \"word2idx_outputs.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    word2idx_outputs = json.load(f)\n",
    "\n",
    "npz = np.load(os.path.join(PREPROC_DIR, \"sequences.npz\"), allow_pickle=True)\n",
    "encoder_input_sequences = list(npz[\"encoder_input_sequences\"])\n",
    "decoder_input_sequences = list(npz[\"decoder_output_sequences\"])  # <sos> + y\n",
    "decoder_target_sequences = list(npz[\"decoder_targets\"])          # y + <eos>\n",
    "\n",
    "# ---------- Longitudes (p99) ----------\n",
    "lens_in  = [len(s) for s in encoder_input_sequences]\n",
    "lens_out = [len(s) for s in decoder_input_sequences]\n",
    "p_in  = max(8, int(np.percentile(lens_in, 99)))\n",
    "p_out = max(8, int(np.percentile(lens_out, 99)))\n",
    "print(f\"p99 longitudes -> input: {p_in} | output: {p_out}\")\n",
    "\n",
    "# ---------- Padding/Truncado ----------\n",
    "Xenc = pad_sequences(encoder_input_sequences, maxlen=p_in,  padding=\"post\", truncating=\"post\", value=0)\n",
    "Xdec = pad_sequences(decoder_input_sequences, maxlen=p_out, padding=\"post\", truncating=\"post\", value=0)\n",
    "Yidx = pad_sequences(decoder_target_sequences, maxlen=p_out, padding=\"post\", truncating=\"post\", value=0)\n",
    "Y    = np.expand_dims(Yidx, -1).astype(\"int32\")   # (N,T,1)\n",
    "\n",
    "# sample_weight: ignora pads\n",
    "sw = (Yidx != 0).astype(\"float32\")  # (N,T)\n",
    "\n",
    "# ---------- Vocabularios ----------\n",
    "num_words_input  = len(word2idx_inputs)\n",
    "num_words_output = len(word2idx_outputs)\n",
    "\n",
    "# ---------- HiperparÃ¡metros ----------\n",
    "EMBED_DIM_ENCODER = 100\n",
    "LATENT_DIM        = 256\n",
    "EMBED_DIM_DECODER = LATENT_DIM     # tying cÃ³modo: D = L\n",
    "DROPOUT_P         = 0.08\n",
    "BATCH_SIZE        = 64\n",
    "EPOCHS_PHASE1     = 30\n",
    "EPOCHS_PHASE2     = 8\n",
    "EPOCHS_PHASE3     = 6\n",
    "\n",
    "# ---------- Embeddings encoder ----------\n",
    "enc_emb_weights_path = os.path.join(EMB_DIR, \"encoder_embedding_matrix_glove100.npy\")\n",
    "use_pretrained_enc = os.path.exists(enc_emb_weights_path)\n",
    "if use_pretrained_enc:\n",
    "    enc_embedding_matrix = np.load(enc_emb_weights_path)\n",
    "    EMBED_DIM_ENCODER = enc_embedding_matrix.shape[1]\n",
    "    print(\"Usando embeddings preentrenados del encoder:\", enc_emb_weights_path)\n",
    "else:\n",
    "    enc_embedding_matrix = None\n",
    "    print(\"Sin matriz preentrenada del encoder (encoder embeddings aleatorios).\")\n",
    "\n",
    "# ---------- Loss: Sparse CE + smoothing ----------\n",
    "def smoothed_sparse_cce(num_classes, label_smoothing=0.02, from_logits=True):\n",
    "    base = keras.losses.CategoricalCrossentropy(\n",
    "        from_logits=from_logits, label_smoothing=label_smoothing\n",
    "    )\n",
    "    def loss_fn(y_true, y_pred):\n",
    "        y_true = tf.cast(tf.squeeze(y_true, axis=-1), tf.int32)  # (N,T)\n",
    "        y_true_oh = tf.one_hot(y_true, depth=num_classes)        # (N,T,C)\n",
    "        return base(y_true_oh, y_pred)\n",
    "    return loss_fn\n",
    "\n",
    "# ---------- Capa de proyecciÃ³n ----------\n",
    "@register_keras_serializable(package=\"Custom\")\n",
    "class TiedOutputProjection(keras.layers.Layer):\n",
    "    def __init__(self, embedding_layer, use_bias=True, temperature=1.0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        # Guardamos referencia al objeto capa de Embedding\n",
    "        self.embedding_layer = embedding_layer\n",
    "        self.use_bias = use_bias\n",
    "        self.temperature = temperature\n",
    "        self.bias = None\n",
    "        self.E = None\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        # Tomamos la variable de embeddings del decoder\n",
    "        self.E = self.embedding_layer.embeddings  # (V, D)\n",
    "        if self.use_bias:\n",
    "            vocab_size = int(self.E.shape[0])\n",
    "            self.bias = self.add_weight(\n",
    "                name=\"bias\", shape=(vocab_size,), initializer=\"zeros\", trainable=True\n",
    "            )\n",
    "        super().build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        logits = tf.linalg.matmul(x, tf.transpose(self.E)) / self.temperature\n",
    "        if self.use_bias:\n",
    "            logits = logits + self.bias\n",
    "        return logits\n",
    "\n",
    "    def get_config(self):\n",
    "        cfg = super().get_config()\n",
    "        cfg.update({\n",
    "            \"use_bias\": self.use_bias,\n",
    "            \"temperature\": self.temperature,\n",
    "        })\n",
    "        return cfg\n",
    "\n",
    "# ---------- ConstrucciÃ³n del modelo ----------\n",
    "# Inputs\n",
    "encoder_inputs = Input(shape=(p_in,),  name=\"encoder_inputs\")\n",
    "decoder_inputs = Input(shape=(p_out,), name=\"decoder_inputs\")\n",
    "\n",
    "# Encoder embedding\n",
    "if use_pretrained_enc:\n",
    "    enc_embedding_layer = Embedding(num_words_input, EMBED_DIM_ENCODER,\n",
    "                                    weights=[enc_embedding_matrix],\n",
    "                                    trainable=False, mask_zero=True, name=\"enc_embedding\")\n",
    "else:\n",
    "    enc_embedding_layer = Embedding(num_words_input, EMBED_DIM_ENCODER,\n",
    "                                    mask_zero=True, name=\"enc_embedding\")\n",
    "enc_emb = enc_embedding_layer(encoder_inputs)\n",
    "\n",
    "# Encoder BiLSTM\n",
    "encoder_bilstm = Bidirectional(\n",
    "    LSTM(LATENT_DIM, return_sequences=True, return_state=True,\n",
    "         dropout=DROPOUT_P, recurrent_dropout=DROPOUT_P),\n",
    "    merge_mode=\"concat\", name=\"encoder_bilstm\"\n",
    ")\n",
    "enc_seq_bi, f_h, f_c, b_h, b_c = encoder_bilstm(enc_emb)     # (None,Tin, 2*L)\n",
    "\n",
    "# Bridge estados -> L\n",
    "bridge_h = Dense(LATENT_DIM, activation=\"tanh\", name=\"bridge_h\")(Concatenate()([f_h, b_h]))\n",
    "bridge_c = Dense(LATENT_DIM, activation=\"tanh\", name=\"bridge_c\")(Concatenate()([f_c, b_c]))\n",
    "\n",
    "# ProyecciÃ³n enc_seq a L para dot attention\n",
    "enc_proj = Dense(LATENT_DIM, name=\"enc_proj\")\n",
    "enc_seq_proj = enc_proj(enc_seq_bi)                            # (None,Tin,L)\n",
    "\n",
    "# Decoder embedding (trainable para tying)\n",
    "decoder_embedding_layer = Embedding(num_words_output, EMBED_DIM_DECODER,\n",
    "                                    mask_zero=True, name=\"dec_embedding\", trainable=True)\n",
    "dec_emb = decoder_embedding_layer(decoder_inputs)              # (None,Tout,L)\n",
    "\n",
    "# Decoder LSTM\n",
    "decoder_lstm = LSTM(LATENT_DIM, return_sequences=True, return_state=True,\n",
    "                    dropout=DROPOUT_P, recurrent_dropout=DROPOUT_P, name=\"decoder_lstm\")\n",
    "dec_seq, _, _ = decoder_lstm(dec_emb, initial_state=[bridge_h, bridge_c])  # (None,Tout,L)\n",
    "\n",
    "# AtenciÃ³n (Luong/dot): query=dec_seq, key/value=enc_seq_proj\n",
    "attn_out = Attention(name=\"attention\")([dec_seq, enc_seq_proj])            # (None,Tout,L)\n",
    "\n",
    "# Fusion + normalizaciÃ³n\n",
    "context = Concatenate(name=\"concat_context\")([dec_seq, attn_out])          # (None,Tout,2L)\n",
    "context = Dense(LATENT_DIM, activation=\"tanh\", name=\"proj_to_emb\")(context)  # (None,Tout,L)\n",
    "context = LayerNormalization(name=\"ln_out\")(context)\n",
    "context = Dropout(DROPOUT_P)(context)\n",
    "\n",
    "tied_proj = TiedOutputProjection(\n",
    "    embedding_layer=decoder_embedding_layer,\n",
    "    use_bias=True,\n",
    "    temperature=0.95,\n",
    "    name=\"tied_output\"\n",
    ")\n",
    "logits = tied_proj(context)\n",
    "\n",
    "model = keras.Model([encoder_inputs, decoder_inputs], logits, name=\"seq2seq_tied_clean\")\n",
    "\n",
    "# MÃ©trica ponderada\n",
    "weighted_acc = keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\")\n",
    "\n",
    "# ---------- FASE 1: encoder embeddings CONGELADOS ----------\n",
    "opt1 = keras.optimizers.Adam(learning_rate=2e-3, clipnorm=1.0)\n",
    "model.compile(\n",
    "    optimizer=opt1,\n",
    "    loss=smoothed_sparse_cce(num_words_output, label_smoothing=0.02, from_logits=True),\n",
    "    metrics=[],\n",
    "    weighted_metrics=[weighted_acc]\n",
    ")\n",
    "ckpt1 = os.path.join(MODEL_DIR, \"best_phase1.keras\")\n",
    "cb1 = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1, min_lr=1e-5),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=4, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(ckpt1, monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
    "]\n",
    "history1 = model.fit(\n",
    "    [Xenc, Xdec], Y,\n",
    "    sample_weight=sw,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS_PHASE1,\n",
    "    validation_split=0.1,\n",
    "    callbacks=cb1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------- FASE 2: descongelar embeddings del encoder ----------\n",
    "if use_pretrained_enc:\n",
    "    enc_embedding_layer.trainable = True\n",
    "    opt2 = keras.optimizers.Adam(learning_rate=5e-4, clipnorm=1.0)\n",
    "    model.compile(\n",
    "        optimizer=opt2,\n",
    "        loss=smoothed_sparse_cce(num_words_output, label_smoothing=0.02, from_logits=True),\n",
    "        metrics=[],\n",
    "        weighted_metrics=[weighted_acc]\n",
    "    )\n",
    "    ckpt2 = os.path.join(MODEL_DIR, \"best_phase2.keras\")\n",
    "    cb2 = [\n",
    "        ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=2, verbose=1, min_lr=1e-5),\n",
    "        EarlyStopping(monitor=\"val_loss\", patience=3, restore_best_weights=True, verbose=1),\n",
    "        ModelCheckpoint(ckpt2, monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
    "    ]\n",
    "    history2 = model.fit(\n",
    "        [Xenc, Xdec], Y,\n",
    "        sample_weight=sw,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        epochs=EPOCHS_PHASE2,\n",
    "        validation_split=0.1,\n",
    "        callbacks=cb2,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "# ---------- FASE 3: descongelar encoder BiLSTM + enc_proj + bridges ----------\n",
    "for layer_name in [\"encoder_bilstm\", \"enc_proj\", \"bridge_h\", \"bridge_c\"]:\n",
    "    model.get_layer(layer_name).trainable = True\n",
    "\n",
    "opt3 = keras.optimizers.Adam(learning_rate=2e-4, clipnorm=1.0)\n",
    "model.compile(\n",
    "    optimizer=opt3,\n",
    "    loss=smoothed_sparse_cce(num_words_output, label_smoothing=0.02, from_logits=True),\n",
    "    metrics=[],\n",
    "    weighted_metrics=[weighted_acc]\n",
    ")\n",
    "ckpt3 = os.path.join(MODEL_DIR, \"best_phase3.keras\")\n",
    "cb3 = [\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=1, verbose=1, min_lr=1e-5),\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=2, restore_best_weights=True, verbose=1),\n",
    "    ModelCheckpoint(ckpt3, monitor=\"val_loss\", save_best_only=True, verbose=1),\n",
    "]\n",
    "history3 = model.fit(\n",
    "    [Xenc, Xdec], Y,\n",
    "    sample_weight=sw,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS_PHASE3,\n",
    "    validation_split=0.1,\n",
    "    callbacks=cb3,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ---------- Guardado final ----------\n",
    "model.save(os.path.join(MODEL_DIR, \"final.keras\"))\n",
    "with open(os.path.join(MODEL_DIR, \"config.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\n",
    "        \"max_input_len\": int(p_in),\n",
    "        \"max_out_len\": int(p_out),\n",
    "        \"num_words_input\": int(num_words_input),\n",
    "        \"num_words_output\": int(num_words_output),\n",
    "        \"latent_dim\": int(LATENT_DIM),\n",
    "        \"embed_dim_encoder\": int(EMBED_DIM_ENCODER),\n",
    "        \"embed_dim_decoder\": int(EMBED_DIM_DECODER),\n",
    "        \"dropout\": float(DROPOUT_P),\n",
    "        \"label_smoothing\": 0.02,\n",
    "        \"temperature_tying\": 0.95,\n",
    "        \"weight_tying\": True,\n",
    "        \"use_pretrained_encoder_embeddings\": bool(use_pretrained_enc),\n",
    "        \"phases\": [EPOCHS_PHASE1, EPOCHS_PHASE2, EPOCHS_PHASE3]\n",
    "    }, f, indent=2)\n",
    "print(\"Modelos guardados en:\", MODEL_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bb8dfd",
   "metadata": {},
   "source": [
    "#### 5 - Inferencia\n",
    "Experimentar el funcionamiento de su modelo. Recuerde que debe realizar la inferencia de los modelos por separado de encoder y decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5dc628e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder_infer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " encoder_inputs_inf (InputL  [(None, 24)]                 0         []                            \n",
      " ayer)                                                                                            \n",
      "                                                                                                  \n",
      " enc_embedding (Embedding)   (None, 24, 100)              329500    ['encoder_inputs_inf[0][0]']  \n",
      "                                                                                                  \n",
      " encoder_bilstm (Bidirectio  [(None, 24, 512),            731136    ['enc_embedding[3][0]']       \n",
      " nal)                         (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256),                                                        \n",
      "                              (None, 256)]                                                        \n",
      "                                                                                                  \n",
      " concat_h_inf (Concatenate)  (None, 512)                  0         ['encoder_bilstm[3][1]',      \n",
      "                                                                     'encoder_bilstm[3][3]']      \n",
      "                                                                                                  \n",
      " concat_c_inf (Concatenate)  (None, 512)                  0         ['encoder_bilstm[3][2]',      \n",
      "                                                                     'encoder_bilstm[3][4]']      \n",
      "                                                                                                  \n",
      " tf.__operators__.ne_2 (TFO  (None, 24)                   0         ['encoder_inputs_inf[0][0]']  \n",
      " pLambda)                                                                                         \n",
      "                                                                                                  \n",
      " enc_proj (Dense)            (None, 24, 256)              131328    ['encoder_bilstm[3][0]']      \n",
      "                                                                                                  \n",
      " bridge_h (Dense)            (None, 256)                  131328    ['concat_h_inf[0][0]']        \n",
      "                                                                                                  \n",
      " bridge_c (Dense)            (None, 256)                  131328    ['concat_c_inf[0][0]']        \n",
      "                                                                                                  \n",
      " tf.cast_2 (TFOpLambda)      (None, 24)                   0         ['tf.__operators__.ne_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1454620 (5.55 MB)\n",
      "Trainable params: 1454620 (5.55 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder_infer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " dec_token_in (InputLayer)   [(None, 1)]                  0         []                            \n",
      "                                                                                                  \n",
      " dec_embedding (Embedding)   multiple                     467200    ['dec_token_in[0][0]']        \n",
      "                                                                                                  \n",
      " state_h_in (InputLayer)     [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " state_c_in (InputLayer)     [(None, 256)]                0         []                            \n",
      "                                                                                                  \n",
      " decoder_lstm (LSTM)         multiple                     525312    ['dec_embedding[3][0]',       \n",
      "                                                                     'state_h_in[0][0]',          \n",
      "                                                                     'state_c_in[0][0]']          \n",
      "                                                                                                  \n",
      " enc_seq_proj_in (InputLaye  [(None, 24, 256)]            0         []                            \n",
      " r)                                                                                               \n",
      "                                                                                                  \n",
      " enc_mask_in (InputLayer)    [(None, 24)]                 0         []                            \n",
      "                                                                                                  \n",
      " attention (Attention)       multiple                     0         ['decoder_lstm[3][0]',        \n",
      "                                                                     'enc_seq_proj_in[0][0]',     \n",
      "                                                                     'enc_mask_in[0][0]']         \n",
      "                                                                                                  \n",
      " concat_ctx_step (Concatena  (None, 1, 512)               0         ['decoder_lstm[3][0]',        \n",
      " te)                                                                 'attention[3][0]']           \n",
      "                                                                                                  \n",
      " proj_to_emb (Dense)         multiple                     131328    ['concat_ctx_step[0][0]']     \n",
      "                                                                                                  \n",
      " ln_out (LayerNormalization  multiple                     512       ['proj_to_emb[3][0]']         \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " tied_output (TiedOutputPro  multiple                     469025    ['ln_out[3][0]']              \n",
      " jection)                                                                                         \n",
      "                                                                                                  \n",
      " softmax_step (Activation)   (None, 1, 1825)              0         ['tied_output[3][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1126177 (4.30 MB)\n",
      "Trainable params: 1126177 (4.30 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "__________________________________________________________________________________________________\n",
      "IN: hello how are you?\n",
      "OUT greedy: i am doing well . how are you ?\n",
      "OUT beam=5: i am doing well . how are you ?\n",
      "OUT sample: well , i am doing well . how are you ?\n",
      "---\n",
      "IN: Do you like going to the beach?\n",
      "OUT greedy: i do not like to go to the beach .\n",
      "OUT beam=5: i do not have a lot of time for that .\n",
      "OUT sample: i do not like to go to the beach .\n",
      "---\n",
      "IN: Do you like to read?\n",
      "OUT greedy: i do not like to read .\n",
      "OUT beam=5: i do not know what to say .\n",
      "OUT sample: i do not like to read . i like to go to the beach\n",
      "---\n",
      "Modelos de inferencia guardados en: ./desafio4/models_infer\n"
     ]
    }
   ],
   "source": [
    "# ---- Diccionarios y tokens especiales\n",
    "SOS, EOS, UNK = \"<sos>\", \"<eos>\", \"<unk>\"\n",
    "PAD_IDX_IN  = 0\n",
    "PAD_IDX_OUT = 0\n",
    "\n",
    "idx2word_outputs = {idx: w for w, idx in word2idx_outputs.items()}\n",
    "idx2word_inputs  = {idx: w for w, idx in word2idx_inputs.items()}\n",
    "sos_id = word2idx_outputs.get(SOS, 1)\n",
    "eos_id = word2idx_outputs.get(EOS, 2)\n",
    "unk_in = word2idx_inputs.get(UNK, 1)\n",
    "\n",
    "# ====================================\n",
    "# 1) Modelo de INFERENCIA del ENCODER\n",
    "# ====================================\n",
    "enc_embedding_layer = model.get_layer(\"enc_embedding\")\n",
    "encoder_bilstm      = model.get_layer(\"encoder_bilstm\")\n",
    "enc_proj_layer      = model.get_layer(\"enc_proj\")\n",
    "bridge_h_layer      = model.get_layer(\"bridge_h\")\n",
    "bridge_c_layer      = model.get_layer(\"bridge_c\")\n",
    "\n",
    "encoder_inputs_inf = Input(shape=(p_in,), name=\"encoder_inputs_inf\")\n",
    "x = enc_embedding_layer(encoder_inputs_inf)\n",
    "enc_seq_bi, f_h, f_c, b_h, b_c = encoder_bilstm(x)\n",
    "\n",
    "# Puentes de estado\n",
    "h_cat = Concatenate(name=\"concat_h_inf\")([f_h, b_h])\n",
    "c_cat = Concatenate(name=\"concat_c_inf\")([f_c, b_c])\n",
    "h0 = bridge_h_layer(h_cat)\n",
    "c0 = bridge_c_layer(c_cat)\n",
    "\n",
    "# Secuencia proyectada para atenciÃ³n\n",
    "enc_seq_proj = enc_proj_layer(enc_seq_bi)\n",
    "\n",
    "# MÃ¡scara booleana del encoder\n",
    "enc_mask = tf.cast(encoder_inputs_inf != 0, tf.bool)\n",
    "\n",
    "encoder_infer = keras.Model(\n",
    "    encoder_inputs_inf, [enc_seq_proj, h0, c0, enc_mask],\n",
    "    name=\"encoder_infer\"\n",
    ")\n",
    "encoder_infer.summary()\n",
    "\n",
    "# ================================================\n",
    "# 2) Modelo de INFERENCIA del DECODER con mÃ¡scara\n",
    "# ================================================\n",
    "dec_embedding_layer = model.get_layer(\"dec_embedding\")\n",
    "decoder_lstm        = model.get_layer(\"decoder_lstm\")\n",
    "attention_layer     = model.get_layer(\"attention\")\n",
    "proj_to_emb_layer   = model.get_layer(\"proj_to_emb\")\n",
    "ln_layer            = model.get_layer(\"ln_out\")\n",
    "tied_output_layer   = model.get_layer(\"tied_output\")\n",
    "\n",
    "# Entradas\n",
    "dec_token_in    = Input(shape=(1,), name=\"dec_token_in\")\n",
    "state_h_in      = Input(shape=(decoder_lstm.units,), name=\"state_h_in\")\n",
    "state_c_in      = Input(shape=(decoder_lstm.units,), name=\"state_c_in\")\n",
    "enc_seq_proj_in = Input(shape=(p_in, decoder_lstm.units), name=\"enc_seq_proj_in\")\n",
    "enc_mask_in     = Input(shape=(p_in,), dtype=\"bool\", name=\"enc_mask_in\")\n",
    "\n",
    "# Paso unario\n",
    "dec_emb_step = dec_embedding_layer(dec_token_in)\n",
    "dec_seq_step, h_out, c_out = decoder_lstm(dec_emb_step, initial_state=[state_h_in, state_c_in])\n",
    "\n",
    "attn_step = attention_layer([dec_seq_step, enc_seq_proj_in], mask=[None, enc_mask_in])\n",
    "\n",
    "# Contexto + proyecciÃ³n a espacio de embedding\n",
    "ctx_step  = Concatenate(name=\"concat_ctx_step\")([dec_seq_step, attn_step])\n",
    "ctx_step  = proj_to_emb_layer(ctx_step)\n",
    "ctx_step  = ln_layer(ctx_step)\n",
    "\n",
    "# Logits atados + softmax\n",
    "logits_step = tied_output_layer(ctx_step)\n",
    "probs_step  = Activation(\"softmax\", name=\"softmax_step\")(logits_step)\n",
    "\n",
    "decoder_infer = keras.Model(\n",
    "    [dec_token_in, state_h_in, state_c_in, enc_seq_proj_in, enc_mask_in],\n",
    "    [probs_step, h_out, c_out],\n",
    "    name=\"decoder_infer\"\n",
    ")\n",
    "decoder_infer.summary()\n",
    "\n",
    "# ===============================\n",
    "# 3) Helpers de preprocesamiento\n",
    "# ===============================\n",
    "def normalize_text(s: str) -> str:\n",
    "    s = s.lower().strip()\n",
    "    s = re.sub(r\"([,;:\\.\\?!])\", r\" \\1 \", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def text_to_input_ids(text: str):\n",
    "    toks = normalize_text(text).split()\n",
    "    ids  = [word2idx_inputs.get(t, unk_in) for t in toks]\n",
    "    return pad_sequences([ids], maxlen=p_in, padding=\"post\", truncating=\"post\", value=PAD_IDX_IN)[0]\n",
    "\n",
    "def ids_to_text(ids):\n",
    "    words = []\n",
    "    for tid in ids:\n",
    "        if tid in (PAD_IDX_OUT, eos_id):\n",
    "            break\n",
    "        words.append(idx2word_outputs.get(int(tid), \"<?>\"))\n",
    "    return \" \".join(words)\n",
    "\n",
    "# =======================\n",
    "# 4) Copy / lexical bias\n",
    "# =======================\n",
    "def compute_copy_bias_ids(enc_ids):\n",
    "    \"\"\"Mapea palabras del input al vocab de salida y devuelve los ids a potenciar.\"\"\"\n",
    "    toks = [idx2word_inputs.get(int(t), None) for t in enc_ids if t != PAD_IDX_IN]\n",
    "    toks = {t for t in toks if t is not None}\n",
    "    out_ids = []\n",
    "    for w in toks:\n",
    "        oid = word2idx_outputs.get(w, None)\n",
    "        if oid is not None:\n",
    "            out_ids.append(int(oid))\n",
    "    return list(set(out_ids))\n",
    "\n",
    "def apply_copy_bias(probs, bias_ids, strength=0.20):\n",
    "    \"\"\"Multiplica por (1+strength) las probs de tokens que aparecen en el input.\"\"\"\n",
    "    if not bias_ids:\n",
    "        return probs\n",
    "    p = probs.copy()\n",
    "    p[bias_ids] *= (1.0 + float(strength))\n",
    "    s = p.sum()\n",
    "    return p / (s + 1e-12)\n",
    "\n",
    "# ====================================\n",
    "# 5) Decoders: Greedy, Sampling, Beam\n",
    "# ====================================\n",
    "def _softmax(x):\n",
    "    x = x - np.max(x)\n",
    "    e = np.exp(x)\n",
    "    return e / (e.sum() + 1e-12)\n",
    "\n",
    "def _apply_min_len_block(probs, ban_ids, t, min_len):\n",
    "    if t < min_len:\n",
    "        probs = probs.copy()\n",
    "        probs[ban_ids] = 0.0\n",
    "        s = probs.sum()\n",
    "        probs = probs if s == 0 else probs / s\n",
    "    return probs\n",
    "\n",
    "def _violates_no_repeat(next_id, generated, n):\n",
    "    if n <= 0 or len(generated) < n-1:\n",
    "        return False\n",
    "    ngram = tuple(generated[-(n-1):] + [next_id])\n",
    "    hist = tuple(generated)\n",
    "    for i in range(len(hist) - (n-1)):\n",
    "        if tuple(hist[i:i+n]) == ngram:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def _filter_top_k(probs, k):\n",
    "    if k is None or k <= 0 or k >= len(probs): return probs\n",
    "    idx = np.argpartition(probs, -k)[-k:]\n",
    "    mask = np.zeros_like(probs, dtype=bool); mask[idx] = True\n",
    "    out = np.where(mask, probs, 0.0); s = out.sum()\n",
    "    return out / (s + 1e-12)\n",
    "\n",
    "def _filter_top_p(probs, p):\n",
    "    if p is None or p <= 0.0 or p >= 1.0: return probs\n",
    "    order = np.argsort(-probs)\n",
    "    cum = np.cumsum(probs[order])\n",
    "    cutoff = order[cum <= p]\n",
    "    if cutoff.size == 0: cutoff = order[:1]\n",
    "    mask = np.zeros_like(probs, dtype=bool); mask[cutoff] = True\n",
    "    out = np.where(mask, probs, 0.0); s = out.sum()\n",
    "    return out / (s + 1e-12)\n",
    "\n",
    "# ----- Greedy (con copy bias) -----\n",
    "def decode_greedy(input_text: str, max_len=None, copy_bias=0.20, verbose=False):\n",
    "    if max_len is None: max_len = p_out\n",
    "    enc_ids = text_to_input_ids(input_text)\n",
    "    enc_seq_proj, h, c, enc_mask = encoder_infer.predict(enc_ids[None, :], verbose=0)\n",
    "    bias_ids = compute_copy_bias_ids(enc_ids)\n",
    "\n",
    "    cur = np.array([[sos_id]], dtype=\"int32\")\n",
    "    out_ids = []\n",
    "    for t in range(max_len):\n",
    "        probs, h, c = decoder_infer.predict([cur, h, c, enc_seq_proj, enc_mask], verbose=0)\n",
    "        probs = probs[0, 0]\n",
    "        probs = apply_copy_bias(probs, bias_ids, strength=copy_bias)\n",
    "        next_id = int(np.argmax(probs))\n",
    "        if verbose:\n",
    "            topk = probs.argsort()[-5:][::-1]\n",
    "            print(f\"t={t} -> {idx2word_outputs.get(next_id)}\",\n",
    "                  [(idx2word_outputs.get(int(k)), float(probs[k])) for k in topk])\n",
    "        if next_id in (eos_id, PAD_IDX_OUT): break\n",
    "        out_ids.append(next_id)\n",
    "        cur[0,0] = next_id\n",
    "    return ids_to_text(out_ids)\n",
    "\n",
    "# ----- Sampling (temp + top-k/top-p + min_len + no_repeat_ngram + copy bias) -----\n",
    "def decode_sampling(input_text: str, max_len=None, temperature=1.2, top_k=40, top_p=0.92,\n",
    "                    min_len=6, no_repeat_ngram=3, copy_bias=0.20, verbose=False):\n",
    "    if max_len is None: max_len = p_out\n",
    "    enc_ids = text_to_input_ids(input_text)\n",
    "    enc_seq_proj, h, c, enc_mask = encoder_infer.predict(enc_ids[None, :], verbose=0)\n",
    "    bias_ids = compute_copy_bias_ids(enc_ids)\n",
    "\n",
    "    cur = np.array([[sos_id]], dtype=\"int32\")\n",
    "    gen = []\n",
    "\n",
    "    for t in range(max_len):\n",
    "        probs, h, c = decoder_infer.predict([cur, h, c, enc_seq_proj, enc_mask], verbose=0)\n",
    "        probs = probs[0, 0]\n",
    "        probs = apply_copy_bias(probs, bias_ids, strength=copy_bias)\n",
    "\n",
    "        # Temperatura\n",
    "        if temperature != 1.0:\n",
    "            logits = np.log(probs + 1e-12) / float(temperature)\n",
    "            probs = _softmax(logits)\n",
    "\n",
    "        # Bloquear EOS/PAD hasta min_len\n",
    "        probs = _apply_min_len_block(probs, ban_ids=[PAD_IDX_OUT, eos_id], t=t, min_len=min_len)\n",
    "\n",
    "        # Filtros top-k/top-p\n",
    "        probs = _filter_top_k(probs, top_k)\n",
    "        probs = _filter_top_p(probs, top_p)\n",
    "\n",
    "        # Evitar n-gramas repetidos\n",
    "        if no_repeat_ngram and len(gen) >= (no_repeat_ngram - 1):\n",
    "            cand_order = np.argsort(-probs)\n",
    "            chosen = None\n",
    "            for k_id in cand_order[:50]:\n",
    "                if not _violates_no_repeat(int(k_id), gen, no_repeat_ngram):\n",
    "                    chosen = int(k_id); break\n",
    "            if chosen is None: chosen = int(cand_order[0])\n",
    "            next_id = chosen\n",
    "        else:\n",
    "            next_id = int(np.random.choice(len(probs), p=probs))\n",
    "\n",
    "        if verbose:\n",
    "            top5 = probs.argsort()[-5:][::-1]\n",
    "            print(f\"t={t} -> {idx2word_outputs.get(next_id)}\",\n",
    "                  [(idx2word_outputs.get(int(k)), float(probs[k])) for k in top5])\n",
    "\n",
    "        if next_id in (eos_id, PAD_IDX_OUT): break\n",
    "        gen.append(next_id)\n",
    "        cur[0, 0] = next_id\n",
    "\n",
    "    return ids_to_text(gen)\n",
    "\n",
    "# ----- Beam Search (length penalty + min_len + no_repeat_ngram + copy bias) -----\n",
    "def decode_beam(input_text: str, beam_size=5, max_len=None, lp_alpha=0.8,\n",
    "                min_len=6, no_repeat_ngram=3, copy_bias=0.20):\n",
    "    if max_len is None: max_len = p_out\n",
    "    enc_ids = text_to_input_ids(input_text)\n",
    "    enc_seq_proj, h0, c0, enc_mask = encoder_infer.predict(enc_ids[None, :], verbose=0)\n",
    "    bias_ids = compute_copy_bias_ids(enc_ids)\n",
    "\n",
    "    beams = [(0.0, [sos_id], h0, c0)]\n",
    "    finished = []\n",
    "\n",
    "    for t in range(max_len):\n",
    "        new_beams = []\n",
    "        for score, seq, h, c in beams:\n",
    "            last = np.array([[seq[-1]]], dtype=\"int32\")\n",
    "            probs, h2, c2 = decoder_infer.predict([last, h, c, enc_seq_proj, enc_mask], verbose=0)\n",
    "            probs = probs[0, 0].copy()\n",
    "            probs = apply_copy_bias(probs, bias_ids, strength=copy_bias)\n",
    "\n",
    "            # min_len: prohibir EOS/PAD\n",
    "            if t < min_len:\n",
    "                probs[PAD_IDX_OUT] = 0.0\n",
    "                probs[eos_id] = 0.0\n",
    "                s = probs.sum(); probs = probs / (s + 1e-12)\n",
    "\n",
    "            # limitar top-k global si beam grande\n",
    "            if beam_size > 5:\n",
    "                probs = _filter_top_k(probs, k=50)\n",
    "\n",
    "            topk = np.argsort(probs)[-beam_size:][::-1]\n",
    "            for k_id in topk:\n",
    "                k_id = int(k_id)\n",
    "                if no_repeat_ngram and _violates_no_repeat(k_id, seq[1:], no_repeat_ngram):\n",
    "                    continue\n",
    "                p = float(probs[k_id] + 1e-12)\n",
    "                new_seq = seq + [k_id]\n",
    "                new_score = score - np.log(p)\n",
    "\n",
    "                if k_id in (eos_id, PAD_IDX_OUT) or t == max_len-1:\n",
    "                    lp = ((5 + len(new_seq)) / 6) ** lp_alpha\n",
    "                    heapq.heappush(finished, (new_score / lp, new_seq))\n",
    "                else:\n",
    "                    new_beams.append((new_score, new_seq, h2, c2))\n",
    "\n",
    "        if not new_beams and finished:\n",
    "            break\n",
    "        beams = sorted(new_beams, key=lambda x: x[0])[:beam_size]\n",
    "\n",
    "    if finished:\n",
    "        best = min(finished, key=lambda x: x[0])[1]\n",
    "    else:\n",
    "        best = beams[0][1]\n",
    "\n",
    "    out = []\n",
    "    for tid in best[1:]:\n",
    "        if tid in (PAD_IDX_OUT, eos_id): break\n",
    "        out.append(tid)\n",
    "    return ids_to_text(out)\n",
    "\n",
    "# ===================\n",
    "# 6) Ejemplos de uso\n",
    "# ===================\n",
    "tests = [\n",
    "    \"hello how are you?\",\n",
    "    \"Do you like going to the beach?\",\n",
    "    \"Do you like to read?\"\n",
    "]\n",
    "for s in tests:\n",
    "    print(\"IN:\", s)\n",
    "    print(\"OUT greedy:\",   decode_greedy(s, copy_bias=0.20))\n",
    "    print(\"OUT beam=5:\",   decode_beam(s, beam_size=5, lp_alpha=0.9, min_len=8, no_repeat_ngram=3, copy_bias=0.20))\n",
    "    print(\"OUT sample:\",   decode_sampling(s, temperature=1.2, top_k=40, top_p=0.92, min_len=8, no_repeat_ngram=3, copy_bias=0.20))\n",
    "    print(\"---\")\n",
    "\n",
    "# ================================\n",
    "# 7) Guardar modelos de inferencia\n",
    "# ================================\n",
    "OUT_DIR = \"./desafio4/models_infer\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "encoder_infer.save(f\"{OUT_DIR}/encoder_infer.keras\")\n",
    "decoder_infer.save(f\"{OUT_DIR}/decoder_infer.keras\")\n",
    "print(\"Modelos de inferencia guardados en:\", OUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
